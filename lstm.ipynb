{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:10.989571Z",
     "iopub.status.busy": "2025-12-09T17:13:10.989034Z",
     "iopub.status.idle": "2025-12-09T17:13:11.013665Z",
     "shell.execute_reply": "2025-12-09T17:13:11.013096Z",
     "shell.execute_reply.started": "2025-12-09T17:13:10.989553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73fd341bb5b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Using the GPU: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Using the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:11.015927Z",
     "iopub.status.busy": "2025-12-09T17:13:11.015236Z",
     "iopub.status.idle": "2025-12-09T17:13:11.101949Z",
     "shell.execute_reply": "2025-12-09T17:13:11.101278Z",
     "shell.execute_reply.started": "2025-12-09T17:13:11.015909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Sentiment\n",
      "0  it’s so adorable that he says “baap” for up an...  positive\n",
      "1  sir i have no words to describe your teaching ...  positive\n",
      "2  the reason they said large and open space inst...   neutral\n",
      "3  for ur information this is an fact that jrntr ...   neutral\n",
      "4  you can really tell the progress awesome espec...  positive\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14691 entries, 0 to 14690\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Comment    14691 non-null  object\n",
      " 1   Sentiment  14691 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 229.7+ KB\n",
      "None\n",
      "Sentiment\n",
      "positive    9121\n",
      "neutral     3700\n",
      "negative    1870\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"dataset/processed/train.csv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.info())\n",
    "print(train_df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Sentiment\n",
      "0  “oh my god guys there’s an octopus eating a cr...  negative\n",
      "1  my daughter will be starting her 8th grade che...  positive\n",
      "2  for some future video you should definitely bu...   neutral\n",
      "3  i’m chronically ill and very frequently find i...  positive\n",
      "4  the pizza planet pizza being awful is just dis...  negative\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3673 entries, 0 to 3672\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Comment    3673 non-null   object\n",
      " 1   Sentiment  3673 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 57.5+ KB\n",
      "None\n",
      "Sentiment\n",
      "positive    2281\n",
      "neutral      925\n",
      "negative     467\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_PATH = \"dataset/processed/test.csv\"\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "print(test_df.head())\n",
    "print(test_df.info())\n",
    "print(test_df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:11.102856Z",
     "iopub.status.busy": "2025-12-09T17:13:11.102677Z",
     "iopub.status.idle": "2025-12-09T17:13:13.542777Z",
     "shell.execute_reply": "2025-12-09T17:13:13.542142Z",
     "shell.execute_reply.started": "2025-12-09T17:13:11.102843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yuweihuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yuweihuang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keep_words = {\n",
    "    \"not\", \"no\", \"nor\",\n",
    "    \"don't\", \"didn't\", \"doesn't\",\n",
    "    \"isn't\", \"wasn't\", \"aren't\", \"weren't\",\n",
    "    \"can't\", \"couldn't\", \"won't\", \"wouldn't\",\n",
    "    \"shouldn't\", \"haven't\", \"hasn't\", \"hadn't\"\n",
    "}\n",
    "\n",
    "stop_words = stop_words - keep_words\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_df['clean_comment'] = train_df['Comment'].astype(str).apply(clean_text)\n",
    "test_df['clean_comment'] = test_df['Comment'].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:13.544911Z",
     "iopub.status.busy": "2025-12-09T17:13:13.544499Z",
     "iopub.status.idle": "2025-12-09T17:13:13.572099Z",
     "shell.execute_reply": "2025-12-09T17:13:13.571499Z",
     "shell.execute_reply.started": "2025-12-09T17:13:13.544891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 11752\n",
      "Val size: 2939\n"
     ]
    }
   ],
   "source": [
    "TEXT_COL = \"clean_comment\"   \n",
    "LABEL_COL = \"Sentiment\"\n",
    "\n",
    "X = train_df[TEXT_COL]\n",
    "y = train_df[LABEL_COL]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_int,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_int\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 3673\n",
      "                                                text  label_int     label\n",
      "0  “ oh god guys ’ octopus eating crab ” watches ...          0  negative\n",
      "1  daughter starting th grade chem section next w...          2  positive\n",
      "2  future video definitely build like huge base o...          1   neutral\n",
      "3  ’ chronically ill frequently find difficult ea...          2  positive\n",
      "4  pizza planet pizza awful disney sticking bit s...          0  negative\n"
     ]
    }
   ],
   "source": [
    "X_test = test_df[TEXT_COL]\n",
    "y_test = le.transform(test_df[LABEL_COL])\n",
    "\n",
    "print(\"Test size:\", len(X_test))\n",
    "\n",
    "preview = pd.DataFrame({\n",
    "    \"text\": X_test.head().values,\n",
    "    \"label_int\": y_test[:5],\n",
    "    \"label\": le.inverse_transform(y_test[:5])\n",
    "})\n",
    "print(preview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenizer + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:13.581357Z",
     "iopub.status.busy": "2025-12-09T17:13:13.581110Z",
     "iopub.status.idle": "2025-12-09T17:13:14.189419Z",
     "shell.execute_reply": "2025-12-09T17:13:14.188809Z",
     "shell.execute_reply.started": "2025-12-09T17:13:13.581304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_words = 20000   \n",
    "max_len   = 150     \n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, num_words, oov_token=\"<OOV>\"):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(text.split())\n",
    "        vocab = counter.most_common(self.num_words - 1)  # reserve 0 for padding, 1 for OOV\n",
    "        self.word_index = {self.oov_token: 1}\n",
    "        idx = 2\n",
    "        for word, _ in vocab:\n",
    "            if idx >= self.num_words:\n",
    "                break\n",
    "            self.word_index[word] = idx\n",
    "            idx += 1\n",
    "        self.index_word = {idx: word for word, idx in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        seqs = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text.split():\n",
    "                idx = self.word_index.get(word)\n",
    "                if idx is None or idx >= self.num_words:\n",
    "                    idx = self.word_index[self.oov_token]\n",
    "                seq.append(idx)\n",
    "            seqs.append(seq)\n",
    "        return seqs\n",
    "\n",
    "def pad_sequences_custom(seqs, maxlen, padding='post', truncating='post'):\n",
    "    padded = np.zeros((len(seqs), maxlen), dtype=np.int64)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if len(seq) > maxlen:\n",
    "            trunc = seq[-maxlen:] if truncating == 'pre' else seq[:maxlen]\n",
    "        else:\n",
    "            trunc = seq\n",
    "        if padding == 'pre':\n",
    "            padded[i, -len(trunc):] = trunc\n",
    "        else:\n",
    "            padded[i, :len(trunc)] = trunc\n",
    "    return padded\n",
    "\n",
    "tokenizer = SimpleTokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "def texts_to_padded(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences_custom(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "X_train_pad = texts_to_padded(X_train)\n",
    "X_val_pad   = texts_to_padded(X_val)\n",
    "X_test_pad  = texts_to_padded(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "cache_path = \"dataset/embeddings/glove-twitter-100.npy\"\n",
    "os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    embedding_matrix = np.load(cache_path)\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    print(f\"Loaded cached embeddings from {cache_path}, dim={embedding_dim}\")\n",
    "else:\n",
    "    glove = api.load(\"glove-twitter-100\")\n",
    "\n",
    "    embedding_dim = glove.vector_size\n",
    "    embedding_matrix = np.random.normal(scale=0.6, size=(max_words, embedding_dim))\n",
    "    embedding_matrix[0] = np.zeros(embedding_dim)\n",
    "\n",
    "    valid = [(w, i) for w, i in tokenizer.word_index.items() if i < max_words]\n",
    "    hits = 0\n",
    "    if valid:\n",
    "        stoi = glove.key_to_index\n",
    "        vectors = glove.vectors\n",
    "        fill_indices = []\n",
    "        vec_indices = []\n",
    "        for w, idx in valid:\n",
    "            key = w if w in stoi else w.lower() if w.lower() in stoi else None\n",
    "            if key is None:\n",
    "                continue\n",
    "            fill_indices.append(idx)\n",
    "            vec_indices.append(stoi[key])\n",
    "        if fill_indices:\n",
    "            embedding_matrix[fill_indices] = vectors[vec_indices]\n",
    "            hits = len(fill_indices)\n",
    "    np.save(cache_path, embedding_matrix)\n",
    "    print(f\"GloVe hits: {hits}/{len(tokenizer.word_index)}\")\n",
    "    print(f\"Saved embeddings to {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Initialize model, loss, dataloader, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with pretrained embeddings frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:14.190407Z",
     "iopub.status.busy": "2025-12-09T17:13:14.190134Z",
     "iopub.status.idle": "2025-12-09T17:13:14.230664Z",
     "shell.execute_reply": "2025-12-09T17:13:14.230123Z",
     "shell.execute_reply.started": "2025-12-09T17:13:14.190380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lstm_units = 128\n",
    "dense_units = 64\n",
    "num_classes = len(le.classes_)\n",
    "vocab_size = max_words\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dense_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            padding_idx=0,\n",
    "            freeze=True,  # set to False to fine-tune embeddings\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, dense_dim)\n",
    "        self.fc2 = nn.Linear(dense_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(emb)\n",
    "        h_cat = torch.cat((h[-2], h[-1]), dim=1)  # concat both directions\n",
    "        x = torch.relu(self.fc1(h_cat))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = BiLSTMClassifier(vocab_size, embedding_dim, lstm_units, dense_units, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "BiLSTMClassifier                         [1, 150]                  [1, 3]                    --\n",
       "├─Embedding: 1-1                         [1, 150]                  [1, 150, 100]             (2,000,000)\n",
       "├─LSTM: 1-2                              [1, 150, 100]             [1, 150, 256]             235,520\n",
       "├─Linear: 1-3                            [1, 256]                  [1, 64]                   16,448\n",
       "├─Dropout: 1-4                           [1, 64]                   [1, 64]                   --\n",
       "├─Linear: 1-5                            [1, 64]                   [1, 3]                    195\n",
       "===================================================================================================================\n",
       "Total params: 2,252,163\n",
       "Trainable params: 252,163\n",
       "Non-trainable params: 2,000,000\n",
       "Total mult-adds (Units.MEGABYTES): 37.34\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.43\n",
       "Params size (MB): 9.01\n",
       "Estimated Total Size (MB): 9.44\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(1, max_len),      # batch, seq_len\n",
    "    dtypes=[torch.long],\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 2.6185383244206775, 1: 1.3234234234234235, 2: 0.5369152046783626}\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)\n",
    "\n",
    "class_weights_arr = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = dict(zip(classes, class_weights_arr))\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "class_weights_tensor = torch.tensor(\n",
    "    [class_weights.get(i, 1.0) for i in range(num_classes)],\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(X_train_pad, dtype=torch.long),\n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    torch.tensor(X_val_pad, dtype=torch.long),\n",
    "    torch.tensor(y_val, dtype=torch.long)\n",
    ")\n",
    "test_ds = TensorDataset(torch.tensor(X_test_pad, dtype=torch.long))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer with cosine annealing and warmup scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "total_steps = num_epochs * len(train_dl)\n",
    "warmup_steps = max(1, int(0.1 * total_steps))\n",
    "\n",
    "def cosine_warmup_lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step + 1) / float(warmup_steps)\n",
    "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, cosine_warmup_lr_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training/validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:14.231551Z",
     "iopub.status.busy": "2025-12-09T17:13:14.231286Z",
     "iopub.status.idle": "2025-12-09T17:13:36.729665Z",
     "shell.execute_reply": "2025-12-09T17:13:36.729099Z",
     "shell.execute_reply.started": "2025-12-09T17:13:14.231530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.9994, val_loss=0.7813, val_acc=0.6553\n",
      "Epoch 2: train_loss=0.7473, val_loss=0.7225, val_acc=0.7292\n",
      "Epoch 3: train_loss=0.6735, val_loss=0.6796, val_acc=0.7169\n",
      "Epoch 4: train_loss=0.6324, val_loss=0.6797, val_acc=0.7336\n",
      "Epoch 5: train_loss=0.6064, val_loss=0.6662, val_acc=0.7125\n",
      "Epoch 6: train_loss=0.5851, val_loss=0.6536, val_acc=0.7349\n",
      "Epoch 7: train_loss=0.5647, val_loss=0.6545, val_acc=0.7322\n",
      "Epoch 8: train_loss=0.5472, val_loss=0.6696, val_acc=0.7332\n",
      "Epoch 9: train_loss=0.5377, val_loss=0.6633, val_acc=0.7397\n",
      "Epoch 10: train_loss=0.5324, val_loss=0.6632, val_acc=0.7404\n"
     ]
    }
   ],
   "source": [
    "best_val = float(\"inf\")\n",
    "\n",
    "MODEL_PATH = \"outputs/lstm/best_lstm.pt\"\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "        running_train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = running_train_loss / len(train_ds)\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            running_val_loss += loss.item() * xb.size(0)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_ds)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test set results and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(20000, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:36.730862Z",
     "iopub.status.busy": "2025-12-09T17:13:36.730656Z",
     "iopub.status.idle": "2025-12-09T17:13:37.455230Z",
     "shell.execute_reply": "2025-12-09T17:13:37.454643Z",
     "shell.execute_reply.started": "2025-12-09T17:13:36.730846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "probas = []\n",
    "with torch.no_grad():\n",
    "    for (xb,) in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        probas.append(torch.softmax(out, dim=1).cpu())\n",
    "\n",
    "y_test_proba = torch.cat(probas, dim=0).numpy()\n",
    "y_test_pred  = np.argmax(y_test_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T17:13:37.501425Z",
     "iopub.status.busy": "2025-12-09T17:13:37.501233Z",
     "iopub.status.idle": "2025-12-09T17:13:37.519711Z",
     "shell.execute_reply": "2025-12-09T17:13:37.519111Z",
     "shell.execute_reply.started": "2025-12-09T17:13:37.501411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7373\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4582    0.7152    0.5585       467\n",
      "     neutral     0.5831    0.6714    0.6241       925\n",
      "    positive     0.9329    0.7685    0.8428      2281\n",
      "\n",
      "    accuracy                         0.7373      3673\n",
      "   macro avg     0.6581    0.7184    0.6751      3673\n",
      "weighted avg     0.7845    0.7373    0.7516      3673\n",
      "\n",
      "\n",
      "One-vs-Rest AUC:\n",
      "AUC for negative (0): 0.9055\n",
      "AUC for neutral (1): 0.8674\n",
      "AUC for positive (2): 0.9128\n",
      "\n",
      "Confusion Matrix (rows = true, cols = predicted):\n",
      "[[ 334   97   36]\n",
      " [ 214  621   90]\n",
      " [ 181  347 1753]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4, target_names=le.classes_))\n",
    "\n",
    "print(\"\\nOne-vs-Rest AUC:\")\n",
    "for idx, cls_name in enumerate(le.classes_):\n",
    "    y_true_bin = (y_test == idx).astype(int)\n",
    "    auc = roc_auc_score(y_true_bin, y_test_proba[:, idx])\n",
    "    print(f\"AUC for {cls_name} ({idx}): {auc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix (rows = true, cols = predicted):\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    \"id\": X_test.index,\n",
    "    \"comment\": test_df.loc[X_test.index, \"Comment\"].values,\n",
    "    \"true_label\": y_test.astype(int),\n",
    "    \"pred_label\": y_test_pred.astype(int),\n",
    "})\n",
    "\n",
    "label_to_id = {cls: idx for idx, cls in enumerate(le.classes_)}\n",
    "for idx, cls in enumerate(le.classes_):\n",
    "    pred_df[f\"prob_{label_to_id[cls]}\"] = y_test_proba[:, idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to outputs/lstm/test_predictions_rs42.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_CSV = f\"outputs/lstm/test_predictions_rs{SEED}.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "pred_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"Saved to\", OUTPUT_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8959600,
     "sourceId": 14074929,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
