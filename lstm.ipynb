{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:10.989571Z",
          "iopub.status.busy": "2025-12-09T17:13:10.989034Z",
          "iopub.status.idle": "2025-12-09T17:13:11.013665Z",
          "shell.execute_reply": "2025-12-09T17:13:11.013096Z",
          "shell.execute_reply.started": "2025-12-09T17:13:10.989553Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bda22f9f590>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Using the GPU: NVIDIA GeForce GTX 1060 6GB\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('Using the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:11.015927Z",
          "iopub.status.busy": "2025-12-09T17:13:11.015236Z",
          "iopub.status.idle": "2025-12-09T17:13:11.101949Z",
          "shell.execute_reply": "2025-12-09T17:13:11.101278Z",
          "shell.execute_reply.started": "2025-12-09T17:13:11.015909Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Comment Sentiment\n",
            "0  it’s so adorable that he says “baap” for up an...  positive\n",
            "1  sir i have no words to describe your teaching ...  positive\n",
            "2  the reason they said large and open space inst...   neutral\n",
            "3  for ur information this is an fact that jrntr ...   neutral\n",
            "4  you can really tell the progress awesome espec...  positive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14691 entries, 0 to 14690\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Comment    14691 non-null  object\n",
            " 1   Sentiment  14691 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 229.7+ KB\n",
            "None\n",
            "Sentiment\n",
            "positive    9121\n",
            "neutral     3700\n",
            "negative    1870\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "TRAIN_DATA_PATH = \"dataset/processed/train.csv\"\n",
        "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
        "\n",
        "print(train_df.head())\n",
        "print(train_df.info())\n",
        "print(train_df['Sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Comment Sentiment\n",
            "0  “oh my god guys there’s an octopus eating a cr...  negative\n",
            "1  my daughter will be starting her 8th grade che...  positive\n",
            "2  for some future video you should definitely bu...   neutral\n",
            "3  i’m chronically ill and very frequently find i...  positive\n",
            "4  the pizza planet pizza being awful is just dis...  negative\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3673 entries, 0 to 3672\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Comment    3673 non-null   object\n",
            " 1   Sentiment  3673 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 57.5+ KB\n",
            "None\n",
            "Sentiment\n",
            "positive    2281\n",
            "neutral      925\n",
            "negative     467\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "TEST_DATA_PATH = \"dataset/processed/test.csv\"\n",
        "test_df = pd.read_csv(TEST_DATA_PATH)\n",
        "\n",
        "print(test_df.head())\n",
        "print(test_df.info())\n",
        "print(test_df['Sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:11.102856Z",
          "iopub.status.busy": "2025-12-09T17:13:11.102677Z",
          "iopub.status.idle": "2025-12-09T17:13:13.542777Z",
          "shell.execute_reply": "2025-12-09T17:13:13.542142Z",
          "shell.execute_reply.started": "2025-12-09T17:13:11.102843Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/yuweihuang/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/yuweihuang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "keep_words = {\n",
        "    \"not\", \"no\", \"nor\",\n",
        "    \"don't\", \"didn't\", \"doesn't\",\n",
        "    \"isn't\", \"wasn't\", \"aren't\", \"weren't\",\n",
        "    \"can't\", \"couldn't\", \"won't\", \"wouldn't\",\n",
        "    \"shouldn't\", \"haven't\", \"hasn't\", \"hadn't\"\n",
        "}\n",
        "\n",
        "stop_words = stop_words - keep_words\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "    # tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_df['clean_comment'] = train_df['Comment'].astype(str).apply(clean_text)\n",
        "test_df['clean_comment'] = test_df['Comment'].astype(str).apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:13.544911Z",
          "iopub.status.busy": "2025-12-09T17:13:13.544499Z",
          "iopub.status.idle": "2025-12-09T17:13:13.572099Z",
          "shell.execute_reply": "2025-12-09T17:13:13.571499Z",
          "shell.execute_reply.started": "2025-12-09T17:13:13.544891Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 11752\n",
            "Val size: 2939\n"
          ]
        }
      ],
      "source": [
        "TEXT_COL = \"clean_comment\"   \n",
        "LABEL_COL = \"Sentiment\"\n",
        "\n",
        "X = train_df[TEXT_COL]\n",
        "y = train_df[LABEL_COL]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_int = le.fit_transform(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_int,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=y_int\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Val size:\", len(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test size: 3673\n",
            "                                                text  label_int     label\n",
            "0  “ oh god guys ’ octopus eating crab ” watches ...          0  negative\n",
            "1  daughter starting th grade chem section next w...          2  positive\n",
            "2  future video definitely build like huge base o...          1   neutral\n",
            "3  ’ chronically ill frequently find difficult ea...          2  positive\n",
            "4  pizza planet pizza awful disney sticking bit s...          0  negative\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df[TEXT_COL]\n",
        "y_test = le.transform(test_df[LABEL_COL])\n",
        "\n",
        "print(\"Test size:\", len(X_test))\n",
        "\n",
        "preview = pd.DataFrame({\n",
        "    \"text\": X_test.head().values,\n",
        "    \"label_int\": y_test[:5],\n",
        "    \"label\": le.inverse_transform(y_test[:5])\n",
        "})\n",
        "print(preview)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Tokenizer + padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:13.581357Z",
          "iopub.status.busy": "2025-12-09T17:13:13.581110Z",
          "iopub.status.idle": "2025-12-09T17:13:14.189419Z",
          "shell.execute_reply": "2025-12-09T17:13:14.188809Z",
          "shell.execute_reply.started": "2025-12-09T17:13:13.581304Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "max_words = 12000\n",
        "\n",
        "from typing import Iterable\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, num_words, oov_token=\"<OOV>\"):\n",
        "        self.num_words = num_words\n",
        "        self.oov_token = oov_token\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "\n",
        "    def fit_on_texts(self, texts: Iterable[str]):\n",
        "        counter = Counter()\n",
        "        for text in texts:\n",
        "            counter.update(text.split())\n",
        "        vocab = counter.most_common(self.num_words - 1)  # reserve 0 for padding, 1 for OOV\n",
        "        self.word_index = {self.oov_token: 1}\n",
        "        idx = 2\n",
        "        for word, _ in vocab:\n",
        "            if idx >= self.num_words:\n",
        "                break\n",
        "            self.word_index[word] = idx\n",
        "            idx += 1\n",
        "        self.index_word = {idx: word for word, idx in self.word_index.items()}\n",
        "\n",
        "    def texts_to_sequences(self, texts: Iterable[str]):\n",
        "        seqs = []\n",
        "        for text in texts:\n",
        "            seq = []\n",
        "            for word in text.split():\n",
        "                idx = self.word_index.get(word)\n",
        "                if idx is None or idx >= self.num_words:\n",
        "                    idx = self.word_index[self.oov_token]\n",
        "                seq.append(idx)\n",
        "            seqs.append(seq)\n",
        "        return seqs\n",
        "\n",
        "def pad_sequences_custom(seqs, maxlen, padding='post', truncating='post'):\n",
        "    padded = np.zeros((len(seqs), maxlen), dtype=np.int64)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        if len(seq) > maxlen:\n",
        "            trunc = seq[-maxlen:] if truncating == 'pre' else seq[:maxlen]\n",
        "        else:\n",
        "            trunc = seq\n",
        "        if padding == 'pre':\n",
        "            padded[i, -len(trunc):] = trunc\n",
        "        else:\n",
        "            padded[i, :len(trunc)] = trunc\n",
        "    return padded\n",
        "\n",
        "def texts_to_padded(texts, tokenizer, max_len):\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences_custom(seqs, maxlen=max_len, padding='post', truncating='post')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Load pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "cache_path = \"dataset/embeddings/glove-twitter-100.npy\"\n",
        "os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
        "\n",
        "\n",
        "def build_embedding_matrix(tokenizer, max_words, cache_dir=\"dataset/embeddings\"):\n",
        "    if os.path.exists(cache_path):\n",
        "        embedding_matrix = np.load(cache_path)\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "        print(f\"Loaded cached embeddings from {cache_path}, dim={embedding_dim}\")\n",
        "        return embedding_matrix.astype(np.float32)\n",
        "\n",
        "    glove = api.load(\"glove-twitter-100\")\n",
        "\n",
        "    embedding_dim = glove.vector_size\n",
        "    embedding_matrix = np.random.normal(scale=0.6, size=(max_words, embedding_dim)).astype(np.float32)\n",
        "    embedding_matrix[0] = np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "    valid = [(w, i) for w, i in tokenizer.word_index.items() if i < max_words]\n",
        "    hits = 0\n",
        "    if valid:\n",
        "        stoi = glove.key_to_index\n",
        "        vectors = glove.vectors\n",
        "        fill_indices = []\n",
        "        vec_indices = []\n",
        "        for w, idx in valid:\n",
        "            key = w if w in stoi else w.lower() if w.lower() in stoi else None\n",
        "            if key is None:\n",
        "                continue\n",
        "            fill_indices.append(idx)\n",
        "            vec_indices.append(stoi[key])\n",
        "        if fill_indices:\n",
        "            embedding_matrix[fill_indices] = vectors[vec_indices]\n",
        "            hits = len(fill_indices)\n",
        "    np.save(cache_path, embedding_matrix)\n",
        "    print(f\"GloVe hits: {hits}/{len(tokenizer.word_index)}\")\n",
        "    print(f\"Saved embeddings to {cache_path}\")\n",
        "    return embedding_matrix.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Initialize model, loss, dataloader, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model with pretrained embeddings frozen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:14.190407Z",
          "iopub.status.busy": "2025-12-09T17:13:14.190134Z",
          "iopub.status.idle": "2025-12-09T17:13:14.230664Z",
          "shell.execute_reply": "2025-12-09T17:13:14.230123Z",
          "shell.execute_reply.started": "2025-12-09T17:13:14.190380Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_classes = len(le.classes_)\n",
        "\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, dense_dim, num_classes, dropout):\n",
        "        super().__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
        "            padding_idx=0,\n",
        "            freeze=True,  # set to False to fine-tune embeddings\n",
        "        )\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, dense_dim)\n",
        "        self.fc2 = nn.Linear(dense_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        _, (h, _) = self.lstm(emb)\n",
        "        h_cat = torch.cat((h[-2], h[-1]), dim=1)  # concat both directions\n",
        "        x = torch.relu(self.fc1(h_cat))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss with class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: {0: 2.618716577540107, 1: 1.3235135135135134, 2: 0.5368928845521325}\n"
          ]
        }
      ],
      "source": [
        "classes = np.unique(y_int)\n",
        "\n",
        "class_weights_arr = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=y_int\n",
        ")\n",
        "\n",
        "class_weights = dict(zip(classes, class_weights_arr))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "class_weights_tensor = torch.tensor(\n",
        "    [class_weights.get(i, 1.0) for i in range(num_classes)],\n",
        "    dtype=torch.float32,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build dataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "def build_loaders(X_pad, y_labels, train_idx, val_idx, batch_size):\n",
        "    train_ds = TensorDataset(\n",
        "        torch.tensor(X_pad[train_idx], dtype=torch.long),\n",
        "        torch.tensor(y_labels[train_idx], dtype=torch.long)\n",
        "    )\n",
        "    val_ds = TensorDataset(\n",
        "        torch.tensor(X_pad[val_idx], dtype=torch.long),\n",
        "        torch.tensor(y_labels[val_idx], dtype=torch.long)\n",
        "    )\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
        "    return train_dl, val_dl\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in dataloader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def eval_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "            y_true.append(yb.cpu())\n",
        "            y_pred.append(pred.cpu())\n",
        "    avg_loss = running_loss / len(dataloader.dataset)\n",
        "    acc = correct / total if total else 0.0\n",
        "    if y_true and y_pred:\n",
        "        y_true_cat = torch.cat(y_true).numpy()\n",
        "        y_pred_cat = torch.cat(y_pred).numpy()\n",
        "        macro_f1 = f1_score(y_true_cat, y_pred_cat, average=\"macro\")\n",
        "    else:\n",
        "        macro_f1 = 0.0\n",
        "    return avg_loss, acc, macro_f1\n",
        "\n",
        "\n",
        "def build_cosine_warmup_scheduler(optimizer, total_steps, warmup_ratio=0.1):\n",
        "    warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step + 1) / float(warmup_steps)\n",
        "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Random search (macro F1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Trial 1/5: {'max_len': 75, 'lstm_units': 96, 'lr': 0.001, 'dropout': 0.3, 'dense_units': 32} ===\n",
            "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n",
            "Trial 1 mean val macro F1: 0.6850\n",
            "=== Trial 2/5: {'max_len': 75, 'lstm_units': 96, 'lr': 0.003, 'dropout': 0.3, 'dense_units': 32} ===\n",
            "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n",
            "Trial 2 mean val macro F1: 0.6971\n",
            "=== Trial 3/5: {'max_len': 150, 'lstm_units': 96, 'lr': 0.002, 'dropout': 0.5, 'dense_units': 64} ===\n",
            "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n",
            "Trial 3 mean val macro F1: 0.6926\n",
            "=== Trial 4/5: {'max_len': 100, 'lstm_units': 96, 'lr': 0.002, 'dropout': 0.5, 'dense_units': 96} ===\n",
            "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n",
            "Trial 4 mean val macro F1: 0.6923\n",
            "=== Trial 5/5: {'max_len': 100, 'lstm_units': 64, 'lr': 0.001, 'dropout': 0.5, 'dense_units': 96} ===\n",
            "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n",
            "Trial 5 mean val macro F1: 0.6845\n",
            "\n",
            "Best params: {'max_len': 75, 'lstm_units': 96, 'lr': 0.003, 'dropout': 0.3, 'dense_units': 32}\n",
            "Best CV macro F1: 0.6971\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterSampler, StratifiedKFold\n",
        "\n",
        "param_dist = {\n",
        "    \"max_len\": [75, 100, 150],\n",
        "    \"lstm_units\": [32, 64, 96],\n",
        "    \"dense_units\": [32, 64, 96],\n",
        "    \"dropout\": [0.3, 0.4, 0.5],\n",
        "    \"lr\": [3e-3, 2e-3, 1e-3]\n",
        "}\n",
        "\n",
        "search_epochs = 10\n",
        "n_iter_search = 5\n",
        "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n",
        "\n",
        "X_all = train_df[TEXT_COL].astype(str).values\n",
        "y_all = y_int\n",
        "\n",
        "best_params = None\n",
        "best_score = -float(\"inf\")\n",
        "trial_history = []\n",
        "\n",
        "for trial_idx, params in enumerate(ParameterSampler(param_dist, n_iter=n_iter_search, random_state=SEED), 1):\n",
        "    print(f\"=== Trial {trial_idx}/{n_iter_search}: {params} ===\")\n",
        "    tokenizer = SimpleTokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(X_all)\n",
        "    X_pad = texts_to_padded(X_all, tokenizer, params[\"max_len\"])\n",
        "    embedding_matrix = build_embedding_matrix(tokenizer, max_words)\n",
        "\n",
        "    fold_f1s = []\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_pad, y_all), 1):\n",
        "        train_dl, val_dl = build_loaders(X_pad, y_all, train_idx, val_idx, batch_size)\n",
        "        model = BiLSTMClassifier(\n",
        "            embedding_matrix,\n",
        "            params[\"lstm_units\"],\n",
        "            params[\"dense_units\"],\n",
        "            num_classes,\n",
        "            params[\"dropout\"]\n",
        "        ).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
        "        total_steps = search_epochs * len(train_dl)\n",
        "        scheduler = build_cosine_warmup_scheduler(optimizer, total_steps)\n",
        "\n",
        "        best_fold_f1 = 0.0\n",
        "        for epoch in range(search_epochs):\n",
        "            train_loss = train_one_epoch(model, train_dl, optimizer, criterion, scheduler)\n",
        "            val_loss, val_acc, val_f1 = eval_model(model, val_dl, criterion)\n",
        "            best_fold_f1 = max(best_fold_f1, val_f1)\n",
        "            #print(f\"Fold {fold_idx} Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_macro_f1={val_f1:.4f}\")\n",
        "        fold_f1s.append(best_fold_f1)\n",
        "\n",
        "    mean_f1 = float(np.mean(fold_f1s)) if fold_f1s else 0.0\n",
        "    trial_history.append({\"params\": params, \"mean_val_macro_f1\": mean_f1})\n",
        "    print(f\"Trial {trial_idx} mean val macro F1: {mean_f1:.4f}\")\n",
        "\n",
        "    if mean_f1 > best_score:\n",
        "        best_score = mean_f1\n",
        "        best_params = params\n",
        "\n",
        "print(\"\\nBest params:\", best_params)\n",
        "print(f\"Best CV macro F1: {best_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Train final model with best params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded cached embeddings from dataset/embeddings/glove-twitter-100.npy, dim=100\n",
            "Final Epoch 1: train_loss=0.9360, val_loss=0.7517, val_acc=0.6798, val_macro_f1=0.6100\n",
            "Final Epoch 2: train_loss=0.7312, val_loss=0.7273, val_acc=0.7322, val_macro_f1=0.6515\n",
            "Final Epoch 3: train_loss=0.6540, val_loss=0.6709, val_acc=0.6999, val_macro_f1=0.6466\n",
            "Final Epoch 4: train_loss=0.6018, val_loss=0.6859, val_acc=0.7387, val_macro_f1=0.6753\n",
            "Final Epoch 5: train_loss=0.5551, val_loss=0.6618, val_acc=0.7390, val_macro_f1=0.6771\n",
            "Final Epoch 6: train_loss=0.5114, val_loss=0.6668, val_acc=0.7397, val_macro_f1=0.6791\n",
            "Final Epoch 7: train_loss=0.4657, val_loss=0.7039, val_acc=0.7326, val_macro_f1=0.6710\n",
            "Final Epoch 8: train_loss=0.4218, val_loss=0.7098, val_acc=0.7506, val_macro_f1=0.6871\n",
            "Final Epoch 9: train_loss=0.3907, val_loss=0.7582, val_acc=0.7550, val_macro_f1=0.6911\n",
            "Final Epoch 10: train_loss=0.3762, val_loss=0.7520, val_acc=0.7526, val_macro_f1=0.6879\n",
            "Best val macro F1: 0.6911 at epoch 9\n",
            "Saved best model to outputs/lstm/best_lstm.pt\n"
          ]
        }
      ],
      "source": [
        "assert best_params is not None, \"Run the random search cell first to set best_params\"\n",
        "\n",
        "best_max_len = best_params[\"max_len\"]\n",
        "best_lstm_units = best_params[\"lstm_units\"]\n",
        "best_dense_units = best_params[\"dense_units\"]\n",
        "best_dropout = best_params[\"dropout\"]\n",
        "best_lr = best_params[\"lr\"]\n",
        "\n",
        "# Fit tokenizer on full training data to align with cached embeddings\n",
        "best_tokenizer = SimpleTokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "best_tokenizer.fit_on_texts(train_df[TEXT_COL].astype(str).values)\n",
        "\n",
        "# Pad splits using the same tokenizer\n",
        "X_train_pad = texts_to_padded(X_train.astype(str).values, best_tokenizer, best_max_len)\n",
        "X_val_pad = texts_to_padded(X_val.astype(str).values, best_tokenizer, best_max_len)\n",
        "X_test_pad = texts_to_padded(test_df[TEXT_COL].astype(str).values, best_tokenizer, best_max_len)\n",
        "\n",
        "# ensure label tensors are numeric arrays\n",
        "_y_train = np.array(y_train, dtype=np.int64)\n",
        "_y_val = np.array(y_val, dtype=np.int64)\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(best_tokenizer, max_words)\n",
        "\n",
        "train_ds = TensorDataset(\n",
        "    torch.tensor(X_train_pad, dtype=torch.long),\n",
        "    torch.tensor(_y_train, dtype=torch.long)\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    torch.tensor(X_val_pad, dtype=torch.long),\n",
        "    torch.tensor(_y_val, dtype=torch.long)\n",
        ")\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "test_ds = TensorDataset(torch.tensor(X_test_pad, dtype=torch.long))\n",
        "test_dl = DataLoader(test_ds, batch_size=256)\n",
        "\n",
        "model = BiLSTMClassifier(\n",
        "    embedding_matrix,\n",
        "    best_lstm_units,\n",
        "    best_dense_units,\n",
        "    num_classes,\n",
        "    best_dropout\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
        "num_epochs_final = 10\n",
        "total_steps_final = num_epochs_final * len(train_dl)\n",
        "scheduler = build_cosine_warmup_scheduler(optimizer, total_steps_final)\n",
        "\n",
        "MODEL_PATH = \"outputs/lstm/best_lstm.pt\"\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "best_val_f1 = -float(\"inf\")\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(num_epochs_final):\n",
        "    train_loss = train_one_epoch(model, train_dl, optimizer, criterion, scheduler)\n",
        "    val_loss, val_acc, val_f1 = eval_model(model, val_dl, criterion)\n",
        "    print(f\"Final Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_macro_f1={val_f1:.4f}\")\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n",
        "print(f\"Best val macro F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
        "print(\"Saved best model to\", MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
              "===================================================================================================================\n",
              "BiLSTMClassifier                         [1, 75]                   [1, 3]                    --\n",
              "├─Embedding: 1-1                         [1, 75]                   [1, 75, 100]              (1,200,000)\n",
              "├─LSTM: 1-2                              [1, 75, 100]              [1, 75, 192]              152,064\n",
              "├─Linear: 1-3                            [1, 192]                  [1, 32]                   6,176\n",
              "├─Dropout: 1-4                           [1, 32]                   [1, 32]                   --\n",
              "├─Linear: 1-5                            [1, 32]                   [1, 3]                    99\n",
              "===================================================================================================================\n",
              "Total params: 1,358,339\n",
              "Trainable params: 158,339\n",
              "Non-trainable params: 1,200,000\n",
              "Total mult-adds (Units.MEGABYTES): 12.61\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.18\n",
              "Params size (MB): 5.43\n",
              "Estimated Total Size (MB): 5.61\n",
              "==================================================================================================================="
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(1, best_max_len),      # batch, seq_len\n",
        "    dtypes=[torch.long],\n",
        "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Test set results and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiLSTMClassifier(\n",
              "  (embedding): Embedding(12000, 100, padding_idx=0)\n",
              "  (lstm): LSTM(100, 96, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc1): Linear(in_features=192, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BiLSTMClassifier(\n",
        "    embedding_matrix,\n",
        "    best_lstm_units,\n",
        "    best_dense_units,\n",
        "    num_classes,\n",
        "    best_dropout\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:36.730862Z",
          "iopub.status.busy": "2025-12-09T17:13:36.730656Z",
          "iopub.status.idle": "2025-12-09T17:13:37.455230Z",
          "shell.execute_reply": "2025-12-09T17:13:37.454643Z",
          "shell.execute_reply.started": "2025-12-09T17:13:36.730846Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "probas = []\n",
        "with torch.no_grad():\n",
        "    for (xb,) in test_dl:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        probas.append(torch.softmax(out, dim=1).cpu())\n",
        "\n",
        "y_test_proba = torch.cat(probas, dim=0).numpy()\n",
        "y_test_pred  = np.argmax(y_test_proba, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-09T17:13:37.501425Z",
          "iopub.status.busy": "2025-12-09T17:13:37.501233Z",
          "iopub.status.idle": "2025-12-09T17:13:37.519711Z",
          "shell.execute_reply": "2025-12-09T17:13:37.519111Z",
          "shell.execute_reply.started": "2025-12-09T17:13:37.501411Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7493\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.4967    0.6403    0.5594       467\n",
            "     neutral     0.5822    0.6854    0.6296       925\n",
            "    positive     0.9178    0.7975    0.8534      2281\n",
            "\n",
            "    accuracy                         0.7493      3673\n",
            "   macro avg     0.6655    0.7077    0.6808      3673\n",
            "weighted avg     0.7797    0.7493    0.7597      3673\n",
            "\n",
            "\n",
            "One-vs-Rest AUC:\n",
            "AUC for negative (0): 0.8918\n",
            "AUC for neutral (1): 0.8630\n",
            "AUC for positive (2): 0.9134\n",
            "\n",
            "Confusion Matrix (rows = true, cols = predicted):\n",
            "[[ 299  115   53]\n",
            " [ 181  634  110]\n",
            " [ 122  340 1819]]\n"
          ]
        }
      ],
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_test_pred, digits=4, target_names=le.classes_))\n",
        "\n",
        "print(\"\\nOne-vs-Rest AUC:\")\n",
        "for idx, cls_name in enumerate(le.classes_):\n",
        "    y_true_bin = (y_test == idx).astype(int)\n",
        "    auc = roc_auc_score(y_true_bin, y_test_proba[:, idx])\n",
        "    print(f\"AUC for {cls_name} ({idx}): {auc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\nConfusion Matrix (rows = true, cols = predicted):\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame({\n",
        "    \"id\": X_test.index,\n",
        "    \"comment\": test_df.loc[X_test.index, \"Comment\"].values,\n",
        "    \"true_label\": y_test.astype(int),\n",
        "    \"pred_label\": y_test_pred.astype(int),\n",
        "})\n",
        "\n",
        "label_to_id = {cls: idx for idx, cls in enumerate(le.classes_)}\n",
        "for idx, cls in enumerate(le.classes_):\n",
        "    pred_df[f\"prob_{label_to_id[cls]}\"] = y_test_proba[:, idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to outputs/lstm/test_predictions_rs42.csv\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_CSV = f\"outputs/lstm/test_predictions_rs{SEED}.csv\"\n",
        "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
        "pred_df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(\"Saved to\", OUTPUT_CSV)\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8959600,
          "sourceId": 14074929,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
